{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-hypothesis",
   "metadata": {
    "executionInfo": {
     "elapsed": 3536,
     "status": "ok",
     "timestamp": 1634420095364,
     "user": {
      "displayName": "Kaitlin Pet",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17883318583265264622"
     },
     "user_tz": 240
    },
    "id": "failing-hypothesis"
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.fftpack\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy import stats\n",
    "import math\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io.wavfile import write\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import minimize\n",
    "import music21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-hebrew",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-raise",
   "metadata": {
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1634420128534,
     "user": {
      "displayName": "Kaitlin Pet",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17883318583265264622"
     },
     "user_tz": 240
    },
    "id": "complex-raise"
   },
   "outputs": [],
   "source": [
    "# Constants for reading .times or .ideal files\n",
    "\n",
    "SECONDS_PER_AT_FRAME = 256/8000\n",
    "\n",
    "fft_size=1024\n",
    "\n",
    "\n",
    "hop_size=int(fft_size/4)\n",
    "sr_ensemble=44100\n",
    "\n",
    "\n",
    "fft_lim =513 #largest bin size for cutoff\n",
    "\n",
    "#X seconds/sample * 512 samples/STFT frame\n",
    "\n",
    "def seconds_to_stft_frames(seconds):\n",
    "    samples = seconds * sr_ensemble\n",
    "    return  math.floor((samples)/hop_size)\n",
    "\n",
    "\n",
    "def stft_frames_to_seconds(stft_frames):\n",
    "    samples = fft_size + (stft_frames-1)*hop_size\n",
    "    return samples/sr_ensemble\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "half_step = 1/12 #multiply this by change to get "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-korea",
   "metadata": {},
   "source": [
    "# User specified Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-withdrawal",
   "metadata": {},
   "source": [
    "Single best volume setting for each instrument. Order is same as instrument list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonvar_mix = np.array([[0.1222869 ],\n",
    "       [0.26452896* .75],\n",
    "       [0.20447949*2],\n",
    "       [0.63013774],\n",
    "       [0.2666939*.75 ],\n",
    "       [1.8622239 ],\n",
    "       [0.322688  ],\n",
    "       [0.3939167* 1.2 ]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-casino",
   "metadata": {
    "id": "engaged-casino"
   },
   "source": [
    "### Files describing original timepoints and unaltered audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-framework",
   "metadata": {
    "id": "matched-rescue"
   },
   "outputs": [],
   "source": [
    "#path to tuning file\n",
    "path_to_ensemble_tun = '../../tuning/mozart_serenade_eflat.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-hello",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1634420132171,
     "user": {
      "displayName": "Kaitlin Pet",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17883318583265264622"
     },
     "user_tz": 240
    },
    "id": "dangerous-hello",
    "outputId": "d5ef3e2f-0e21-43b0-b6c2-b455d6edf0a5"
   },
   "outputs": [],
   "source": [
    "path_to_ensemble_times=\"../../orchestra_part_times/mozart_serenade_eflat.\" #partial path for parsing\n",
    "\n",
    "path_to_ensemble_audio=\"../../orchestra_part_audio/mozart_serenade_eflat.\" #partial path for parsing\n",
    "\n",
    "instrument_lst = [\"oboe_1\", \n",
    "                  \"oboe_2\", \n",
    "                  \"clarinet_1\", \n",
    "                  \"clarinet_2\", \n",
    "                  \"bassoon_1\", \n",
    "                  \"bassoon_2\", \n",
    "                  \"horn_in_e_1\", \n",
    "                  \"horn_in_e_2\"]\n",
    "\n",
    "#number of parts\n",
    "num_voices=8\n",
    "#sanity check\n",
    "num_voices == len(instrument_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-madagascar",
   "metadata": {},
   "source": [
    "Path to midi specification. Order of parts may be different in midi when read in by music21, if so use mid_instrument_lst variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mid =\"../../mozart_serenade_eflat/mozart_serenade_eflat.mid\"\n",
    "mid_instrument_lst= ['oboe_1',\n",
    " 'oboe_2',\n",
    " 'clarinet_1',\n",
    " 'clarinet_2',\n",
    "                      'horn_in_e_1',\n",
    " 'horn_in_e_2',\n",
    " 'bassoon_1',\n",
    " 'bassoon_2',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-pacific",
   "metadata": {
    "id": "dense-pacific"
   },
   "source": [
    "Load Original Parts Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-professor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28219,
     "status": "ok",
     "timestamp": 1634420160637,
     "user": {
      "displayName": "Kaitlin Pet",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17883318583265264622"
     },
     "user_tz": 240
    },
    "id": "burning-professor",
    "outputId": "2a05e3f2-ebea-41cb-8ecf-c4ae683f1622"
   },
   "outputs": [],
   "source": [
    "#Load each file into a list\n",
    "ensemble_audio_lst = []\n",
    "sr_dct={}\n",
    "for instr in instrument_lst:\n",
    "    print(instr)\n",
    "    samples, sr = librosa.load(path_to_ensemble_audio+instr+\".wav\", sr=None)\n",
    "    if sr==sr_ensemble:\n",
    "        samples_48k=samples\n",
    "    else:\n",
    "        samples_48k = librosa.resample(samples, sr, sr_ensemble, 'kaiser_fast')\n",
    "    ensemble_audio_lst.append(samples_48k)\n",
    "\n",
    "\n",
    "#should be (num_instruments, length_samples)\n",
    "ensemble_audio_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-diameter",
   "metadata": {},
   "source": [
    "## Create list of STFTs for each part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-provision",
   "metadata": {
    "executionInfo": {
     "elapsed": 5664,
     "status": "ok",
     "timestamp": 1634420166282,
     "user": {
      "displayName": "Kaitlin Pet",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17883318583265264622"
     },
     "user_tz": 240
    },
    "id": "rapid-provision"
   },
   "outputs": [],
   "source": [
    "#\n",
    "part_stfts = [ ]\n",
    "\n",
    "for i in  range(0, num_voices):\n",
    "    part_stfts.append(\n",
    "        librosa.stft(ensemble_audio_lst[i], n_fft=fft_size, hop_length=hop_size, center=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-empire",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1634420166283,
     "user": {
      "displayName": "Kaitlin Pet",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17883318583265264622"
     },
     "user_tz": 240
    },
    "id": "demographic-empire",
    "outputId": "ea02e61f-097a-4e7a-fb3f-30cf3caf8e27"
   },
   "outputs": [],
   "source": [
    "part_stfts[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-wilson",
   "metadata": {
    "id": "optimum-wilson"
   },
   "source": [
    "### Extract data from each times file and tunign file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use -1 notation on pickup\n",
    "def eval_measure_fun(measure_str, quarter_per_measure = 4, pickup = False):\n",
    "    mixed_lst = measure_str.split('+')\n",
    "    measures = eval(mixed_lst[0])\n",
    "    beats = eval(mixed_lst[1])\n",
    "    measure_len = quarter_per_measure * .25 #assume quarter is .25\n",
    "    \n",
    "    return measures*measure_len + beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_dct={} #list with dataframe corresponding to each one \n",
    "\n",
    "\n",
    "\n",
    "for instr in instrument_lst:\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Read in .times file as dataframe\n",
    "    '''\n",
    "    times_df = pd.read_csv(path_to_ensemble_times+instr+\".times\", \n",
    "                              header=None,\n",
    "                              delim_whitespace=True,\n",
    "                              names=['measure', 'AT', 'marked'])\n",
    "\n",
    "\n",
    "    '''\n",
    "    Read in tuning file\n",
    "    '''\n",
    "    tun_df = pd.read_csv(path_to_ensemble_tun+instr+\".tun\", \n",
    "                              header=None,\n",
    "                              delim_whitespace=True,\n",
    "                              names=['measure','shift (half step)'])\n",
    "\n",
    "    #parse out the word 'solo' so can join with times df\n",
    "    remove_solo = lambda x: x[5:len(x)]\n",
    "\n",
    "    tun_df['measure']=list(map(remove_solo, tun_df['measure']))\n",
    "    \n",
    "    '''\n",
    "    get absolute pitch shift by multiplying by half step constant \n",
    "    '''\n",
    "    get_abs_tun = lambda x: 2 ** (-1*x*half_step)#Guessing positive value -> flatter\n",
    "    \n",
    "    tun_df['tune (abs)'] = list(map(get_abs_tun, tun_df['shift (half step)']))\n",
    "\n",
    "    '''\n",
    "    Don't care about marked\n",
    "    '''\n",
    "    times_df = times_df[['measure', 'AT']]\n",
    "\n",
    "    '''\n",
    "    Remove extra rows\n",
    "    '''\n",
    "\n",
    "    times_df=times_df[4:len(times_df)-1].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    Join\n",
    "    '''\n",
    "    times_df= times_df.set_index('measure').join(tun_df.set_index('measure')).reset_index()\n",
    "    \n",
    "    '''\n",
    "    use mapping to cast measure\n",
    "    '''\n",
    "\n",
    "    evaluatedMeasure=list(map(eval_measure_fun, times_df['measure']))\n",
    "\n",
    "    times_df['eval_measure']=evaluatedMeasure        \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    use mapping to AT to float\n",
    "    '''\n",
    "    \n",
    "    evalfun =lambda x: eval(x)\n",
    "    try:\n",
    "        evaluatedsec=list(map(evalfun, times_df['AT']))\n",
    "        times_df['AT']=evaluatedsec\n",
    "    except:\n",
    "        print(\"casting done automatically by pandas\")\n",
    "\n",
    "    '''\n",
    "    Pad last element AT (so it doesn't cut off)\n",
    "    '''\n",
    "    times_df['AT'].iloc[-1]= times_df.iloc[-1]['AT']+30\n",
    "\n",
    "    '''\n",
    "    Convert AT to seconds\n",
    "    '''\n",
    "    times_df['Seconds']=times_df['AT']*SECONDS_PER_AT_FRAME \n",
    "\n",
    "    '''\n",
    "    Get STFT frame number at each timepoint\n",
    "    '''\n",
    "\n",
    "    evaluatedstft=list(map(seconds_to_stft_frames, times_df['Seconds']))\n",
    "      \n",
    "\n",
    "    times_df['STFT frames']=evaluatedstft\n",
    "\n",
    "    '''\n",
    "    Add to dictionary\n",
    "    '''\n",
    "    df_dct[instr]=times_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-smell",
   "metadata": {},
   "source": [
    "# Use midi file to remove \"ghost notes\", i.e. additional time points not corresponding to note onsets in the .times file. \n",
    "\n",
    "Note this has downside of removing note offsets, if offset is not in the combined rhythm set note will just keep going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For a given part gets music21 notes/chords, measure metric used in Cadenza, \n",
    "    and each note's offset in quarter notes\n",
    "Params:\n",
    "part: music21.stream.Part, part to parse\n",
    "\n",
    "Returns: \n",
    "\n",
    "list of music21 notes/chords, measure metric used in Cadenza, \n",
    "    and each note's offset in quarter notes\n",
    "\n",
    "Returns :\n",
    "offsets, eval_measures, notes- lists of.....\n",
    "    'notes': music21 note/chords\n",
    "    'evaluated_measure' : used in Cadenza\n",
    "    'beat position' : note offset in quarter note beats \n",
    "'''\n",
    "def extract_info_from_part(part):\n",
    "    offsets=[] #offset in beats\n",
    "    eval_measures=[] #same form as cadenza\n",
    "    notes=[] #music21 note or chord\n",
    "    cur_measure_offset=0\n",
    "\n",
    "    for measure in part.getElementsByClass(music21.stream.Measure):\n",
    "        \n",
    "        if measure.timeSignature != None:\n",
    "            cur_ts = measure.timeSignature\n",
    "            beatVal = 2 **(2-math.log(cur_ts.denominator)/math.log(2))\n",
    "            num_beats = cur_ts.numerator\n",
    "            measure_len = beatVal * num_beats\n",
    "\n",
    "        \n",
    "        \n",
    "        for note in list(measure.recurse().notes):\n",
    "            offsets.append(cur_measure_offset + note.offset)\n",
    "            eval_measures.append(measure.number + note.offset/4)\n",
    "            notes.append(note)\n",
    "        cur_measure_offset += measure_len\n",
    "    \n",
    "    return offsets, eval_measures, notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = music21.converter.parse(path_to_mid)\n",
    "b.show('text')\n",
    "\n",
    "\n",
    "parts = b.getElementsByClass(music21.stream.Part)\n",
    "\n",
    "for i in range(len(parts)):\n",
    "    part_info= extract_info_from_part(parts[i])\n",
    "    keep_times = [float(time) for time in part_info[1]]\n",
    "    instr_df = df_dct[mid_instrument_lst[i]]\n",
    "    #append last time \n",
    "    keep_times.append(instr_df['eval_measure'].iloc[-1])\n",
    "    locations = instr_df[instr_df['eval_measure'].isin(keep_times)].index\n",
    "    new_df = instr_df.loc[locations].reset_index(drop=True)\n",
    "    print(part_info[2])\n",
    "    print(new_df)\n",
    "    df_dct[mid_instrument_lst[i]]=new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-birthday",
   "metadata": {},
   "source": [
    "## Audio Stretching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_shift_stretch1(audio_samples_full, start_sample, \n",
    "                         end_sample, prev_phase, sr, \n",
    "                         shift_constant, \n",
    "                         desired_frame_num, \n",
    "                         first_note, fft_size, hop_size, window, fft_height):\n",
    "\n",
    "    \n",
    "    '''\n",
    "    We want audio from start of current frame to fft_size past ending point to calculate phases.\n",
    "    Will only use last frame to calculate phase advance - will not save modulus\n",
    "    \n",
    "      note start \n",
    "       |----\n",
    "         ----\n",
    "          ---- \n",
    "           ---- note end\n",
    "            ----|\n",
    "             ---|-\n",
    "              --|--\n",
    "               -|---\n",
    "                |----\n",
    "    '''\n",
    "    \n",
    "    audio_samples = audio_samples_full[start_sample:end_sample+fft_size*2] \n",
    "    #** fft_size *2 gives some buffer audio for ptich shifting (want at least fft_size)\n",
    "    #print(\"shape of audio_samples is \",audio_samples.shape)\n",
    "    \n",
    "    '''\n",
    "    Part I: Perform Pitch shift on all audio \n",
    "    '''\n",
    "    #print(\"fft_size \", fft_size)\n",
    "    #print(\"hop size\", hop_size)\n",
    "    N_note=len(audio_samples)\n",
    "    total_time = N_note/sr_ensemble\n",
    "    \n",
    "    #original timepoints\n",
    "    x = np.linspace(0, total_time, num=N_note, endpoint=True)\n",
    "\n",
    "    #interpolation function \n",
    "    sample_interpolation = interp1d(x, audio_samples, kind='linear')\n",
    "\n",
    "    #New timepoints\n",
    "    xshift = np.linspace(0, total_time, num=int(N_note/shift_constant), endpoint=True) \n",
    "    #apply interpolation function to new timepoints\n",
    "    yshift = sample_interpolation(xshift)\n",
    "\n",
    "    '''\n",
    "    Part II: Time stretch\n",
    "    \n",
    "    We want new audio to occupy desired_frame_num amount of frames \n",
    "    Generate  desired_frames+1 timepoints from [start_sample to end_sample]\n",
    "    For each timepoint except first, phase differential is difference between phase of current timepoint fft \n",
    "                and phase of current timepoint - (hop size?) fft \n",
    "                ** test what happens when this is previous index fft differential \n",
    "    \n",
    "    \n",
    "    If we are at beginning, initial phase is preserved as first phase difference\n",
    "    Else, we are plugging in the last calculated phase difference of previous note\n",
    "    '''\n",
    "    \n",
    "    #timpoints  \n",
    "    N_shifted = int((end_sample-start_sample)/shift_constant) #want start of first note to start of next_note \n",
    "    timepoints = np.linspace(0, N_shifted, num=desired_frame_num+1, endpoint=True).round(0).astype(int)\n",
    "\n",
    "    #iterate through creating frames - making 1 extra \n",
    "    mod = np.zeros((int(fft_size/2)+1, desired_frame_num+1))\n",
    "    diff_phase = np.zeros(mod.shape)\n",
    "\n",
    "    for i in range(desired_frame_num+1): \n",
    "\n",
    "        #Get position from timepoints \n",
    "        sample_index = timepoints[i]\n",
    "        \n",
    "        #Create fft starting from sample index \n",
    "        chunk1start=sample_index\n",
    "        chunk1end=chunk1start+fft_size\n",
    "        chunk1 = yshift[chunk1start:chunk1end] *window\n",
    "        fft1= fft(chunk1)\n",
    "        \n",
    "        #Mod\n",
    "        mod[:,i]=np.abs(fft1[0:fft_height ])\n",
    "\n",
    "        #Phase \n",
    "        \n",
    "        if i==0:\n",
    "            \n",
    "            #if it's the first note, keep its current phase \n",
    "            if first_note == True:\n",
    "                frame_phase = np.angle(fft1)\n",
    "                diff_phase[:,0] = frame_phase[0:fft_height ]\n",
    "            #otherwise sub in last phase advance \n",
    "            else:\n",
    "                diff_phase[:,0:1] = prev_phase\n",
    "        else:\n",
    "            #phase differential is calculated based on fft starting at cur_position - hop_size \n",
    "            chunk2start = sample_index-hop_size\n",
    "                \n",
    "            chunk2end = chunk2start+fft_size\n",
    "            #If we're going sharp, first value will be less than 0\n",
    "            if chunk2start <0:\n",
    "                chunk2=np.zeros(fft_size)\n",
    "                chunk2[0-chunk2start:]=yshift[0:chunk2end]\n",
    "                chunk2=chunk2*window\n",
    "            #normal case \n",
    "            else:\n",
    "                chunk2 = yshift[chunk2start : chunk2end]*window\n",
    "            fft2 = fft(chunk2) \n",
    "            \n",
    "            #take difference \n",
    "            frame_phase = np.angle(fft1)[0:fft_height ]-np.angle(fft2)[0:fft_height ] \n",
    "            diff_phase[:,i]=frame_phase\n",
    "\n",
    "    #Remove last element, saving the phase advance\n",
    "    \n",
    "    joining_phase_advance = diff_phase[:, -1:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    diff_phase = diff_phase [:, 0:-1]\n",
    "    mod=mod[:, 0:-1]\n",
    "    \n",
    "    return mod, diff_phase, joining_phase_advance\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocode_to_new_frames(df, part_samples, new_end_time):\n",
    "    #new end time and new end frame should be same for everyone (if not some can be cut )\n",
    "\n",
    "     #constants\n",
    "    new_end_frame = seconds_to_stft_frames(new_end_time)\n",
    "    window=scipy.signal.windows.hann(fft_size)\n",
    "    fft_height = int(fft_size/2+1) #height of fft (# bins)\n",
    "\n",
    "\n",
    "    #fillable arrays\n",
    "    vocoded_mod= np.zeros((fft_height, new_end_frame))\n",
    "    vocoded_diff_phase= np.zeros((fft_height, new_end_frame))\n",
    "\n",
    "    start_og_sample = int(df.iloc[0]['Seconds']*sr_ensemble)#Audio frame in original playing begins\n",
    "    start_new_frame = df.iloc[0]['New STFT frames']\n",
    "\n",
    "    num_measures = len(df)\n",
    "    for i in range(0, num_measures-1):\n",
    "\n",
    "        end_og_sample = int(df.iloc[i+1]['Seconds']*sr_ensemble)\n",
    "        end_new_frame = df.iloc[i+1]['New STFT frames']\n",
    "\n",
    "        #print(start_og_sample, \"start og sample\")\n",
    "        #print(end_og_sample, \"end og sample\")\n",
    "\n",
    "\n",
    "        pitch_shift = df.iloc[i]['tune (abs)']\n",
    "        #print(\"************\\n\"+df.iloc[i]['measure'])\n",
    "        #print(\"shifting to pitch \", pitch_shift)\n",
    "        #Calculate nessesary time stretch\n",
    "        if end_og_sample < start_og_sample:\n",
    "            print(\"END OG FRAME IS LESS THAN START OG FRAME\")\n",
    "            print(start_og_sample)\n",
    "            print(end_og_sample)\n",
    "\n",
    "        new_frame_dif = end_new_frame - start_new_frame\n",
    "\n",
    "\n",
    "        '''\n",
    "        Changes\n",
    "\n",
    "        Hypothesis from test 2: just STFT process resutls in 1 more? frame than expected\n",
    "        Resutl: \n",
    "        '''        \n",
    "\n",
    "\n",
    "        #if we are on first chord, preserve initial phase \n",
    "        if i==0:\n",
    "\n",
    "\n",
    "            mod, diff_phase, end_phase = pitch_shift_stretch1(audio_samples_full=part_samples,\n",
    "                                                              start_sample=start_og_sample, end_sample=end_og_sample, \n",
    "                                                              prev_phase=None, sr=sr_ensemble, shift_constant=pitch_shift, \n",
    "                                                              desired_frame_num = new_frame_dif, first_note=True, \n",
    "                                                              fft_size=fft_size, hop_size=hop_size, \n",
    "                                                              window=window, fft_height=fft_height)\n",
    "\n",
    "        else:\n",
    "\n",
    "\n",
    "            mod, diff_phase, end_phase = pitch_shift_stretch1(audio_samples_full=part_samples,\n",
    "                                                              start_sample=start_og_sample, end_sample=end_og_sample, \n",
    "                                                              prev_phase=end_phase, sr=sr_ensemble, shift_constant=pitch_shift, \n",
    "                                                              desired_frame_num = new_frame_dif, first_note=False, \n",
    "                                                              fft_size=fft_size, hop_size=hop_size, \n",
    "                                                              window=window, fft_height=fft_height)\n",
    "\n",
    "\n",
    "        correct_len = end_new_frame-start_new_frame\n",
    "        #print(\"length should be \", correct_len)\n",
    "        #print(\"new chunk shape\", mod.shape[1])\n",
    "\n",
    "\n",
    "        #Put vocoded chunk in final STFT\n",
    "\n",
    "        vocoded_mod[:,start_new_frame:end_new_frame]=mod\n",
    "        vocoded_diff_phase[:,start_new_frame:end_new_frame]=diff_phase\n",
    "        \n",
    "        #increment samples\n",
    "\n",
    "        start_og_sample = end_og_sample\n",
    "        start_new_frame = end_new_frame\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return vocoded_mod, vocoded_diff_phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_final_parts(ensemble_audio_lst,df_dct,instrument_lst, end_pad ): #end pad is in ms\n",
    "    adj_part_lst = []\n",
    "    new_end_time = df_dct['oboe_1'].iloc[len(df_dct['oboe_1'])-1]['new_times']+end_pad\n",
    "    for i in range(0, len(instrument_lst)):\n",
    "        print(\"***************************************************************************************************\")\n",
    "        print(\"Instrument: \",instrument_lst[i] )\n",
    "        #get correct stft and dataframe to describe instrument\n",
    "        part_samples=ensemble_audio_lst[i]\n",
    "        df = df_dct[instrument_lst[i]]\n",
    "        #vocode part so it fits the new timing scheme\n",
    "        vocoded_mod, vocoded_diff_phase = vocode_to_new_frames(df, part_samples, new_end_time)\n",
    "        cum_resampled_phase = np.cumsum(vocoded_diff_phase, axis=1)\n",
    "\n",
    "        #combine modulus and argument to make final stft\n",
    "        cum_stft = vocoded_mod*np.exp(1j*cum_resampled_phase) \n",
    "\n",
    "        #add to list\n",
    "        adj_part_lst.append(cum_stft)\n",
    "    return adj_part_lst    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_parts(adj_part_lst):\n",
    "        #sum parts \n",
    "    sum_vocoded_stft= np.zeros((int(fft_size/2+1), adj_part_lst[0].shape[1]), dtype='complex')\n",
    "    for i in range(0, len(instrument_lst)):\n",
    "        #add waves\n",
    "        sum_vocoded_stft += adj_part_lst[i]*nonvar_mix[i]\n",
    "        \n",
    "    #convert to audio sphere\n",
    "    adjusted_by_measure = librosa.istft(sum_vocoded_stft,  hop_length=hop_size)\n",
    "    return adjusted_by_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mix_parts(adj_part_lst, nonvar_mix, folder_path = None):\n",
    "    \n",
    "    if folder_path==None:\n",
    "        print(\"Must supply path of folder to write parts. Create first\")\n",
    "        return 0\n",
    "    print(\"writing to \"+folder_path)    \n",
    "    #list of parts   \n",
    "    part_samples_lst = []\n",
    "    \n",
    "    for i in range(0, len(instrument_lst)):\n",
    "        #convert into sample space\n",
    "        samples_float = librosa.istft(adj_part_lst[i]*nonvar_mix[i],hop_length=hop_size)\n",
    "        samples_int = (2**16*samples_float).astype(np.int16)\n",
    "        part_samples_lst.append(samples_int)\n",
    "        print(\"writing part for \", instrument_lst[i])\n",
    "        write(folder_path+\"/\"+instrument_lst[i]+\".wav\", sr_ensemble, samples_int)\n",
    "        \n",
    "\n",
    "    return part_samples_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-halloween",
   "metadata": {},
   "source": [
    "# Look at Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-membership",
   "metadata": {},
   "source": [
    "# Create df with all eval_measure and STFT frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_STFT_dct={}\n",
    "\n",
    "for v in instrument_lst:\n",
    "    df_STFT_dct[v]= df_dct[v][['eval_measure', 'STFT frames']]\n",
    "    df_STFT_dct[v][v]=df_STFT_dct[v]['STFT frames']\n",
    "    df_STFT_dct[v]=df_STFT_dct[v][['eval_measure', v]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = df_STFT_dct['oboe_1'].set_index('eval_measure')\n",
    "\n",
    "for v in instrument_lst[1:]:\n",
    "    comb_df = comb_df.join(\n",
    "    df_STFT_dct[v].set_index('eval_measure'), \n",
    "    #on = 'eval_measure', \n",
    "    how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = comb_df.reset_index()\n",
    "comb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-wagon",
   "metadata": {},
   "source": [
    "### Extrapolate missing values for mean calculation\n",
    "\n",
    "- Want tempo in interpolated sections to accelerate from tempo_old to tempo_new. \n",
    "- Method 1: Avg tempo\n",
    "    - Extrapolate forward from prev section and backward from next section. Average times. \n",
    "- Method 2: Weighted avg tempo\n",
    "    - Closer to forward implies tempo closer to forward, closer to backward implies tempo closer to backward. Constants calculated based from measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part measures is df_dct[v]['eval_measure']\n",
    "#max_dist is maximum gap between adjacent notes (in measures)\n",
    "'''\n",
    "Aim: everything under 2 measures is interpolated linearly, otherwise tempo extrapolated \n",
    "'''\n",
    "def find_blocks(part_measures, max_dist = 2):\n",
    "    block_lst = []\n",
    "    cur_block = []\n",
    "    for i in range(len(part_measures)-1):\n",
    "\n",
    "        cur_measure = part_measures[i]\n",
    "        cur_block.append(i)\n",
    "        #check distance from next block\n",
    "        next_measure = part_measures[i+1]\n",
    "\n",
    "        if next_measure - cur_measure > max_dist : \n",
    "            #single notes do not constitute block\n",
    "            if len(cur_block)>1:\n",
    "                block_lst.append(cur_block)\n",
    "            cur_block = []\n",
    "    if len(cur_block)>0:\n",
    "        cur_block.append(len(part_measures)-1) #add last element\n",
    "        block_lst.append(cur_block)\n",
    "    return block_lst   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_extrapolate_df(nan_df, df_dct, all_measures, method = \"avg\"):\n",
    "    extrap_df = nan_df.copy()\n",
    "\n",
    "    for v in instrument_lst:\n",
    "        x = np.array(df_dct[v]['eval_measure'])\n",
    "        print(x)\n",
    "        y = np.array(df_dct[v]['STFT frames']) \n",
    "        blocks = find_blocks(x)\n",
    "        \n",
    "        print(\"BLOCKS\\n\", blocks)\n",
    "        #store extrapolations\n",
    "        forward_extrapolation_lst = []\n",
    "        backward_extrapolation_lst =[]\n",
    "        for i in range(len(blocks)):\n",
    "            print(\"\\n******************************\\nOn block\", i, \"\\n*******\")\n",
    "            block = blocks[i]\n",
    "            '''\n",
    "            1. Interpolate within blocks to get locations of local rests\n",
    "            '''\n",
    "            #fit function\n",
    "            \n",
    "            print(\"fitting: \", x[block], \"to\", y[block])\n",
    "            f = interp1d(x[block], y[block] , fill_value = 'extrapolate')\n",
    "            #figure out combined index locations \n",
    "            start_measure = x[block[0]]\n",
    "            end_measure = x[block[-1]]\n",
    "            print(\"starting on measure\", start_measure, \"ending on measure\", end_measure)\n",
    "            start_comb_idx = all_measures.index(start_measure)\n",
    "            end_comb_idx = all_measures.index(end_measure)\n",
    "            print('In full version starts on index', start_comb_idx, \"and ends on index\",  end_comb_idx)\n",
    "            #perform interpolation within block \n",
    "            y1 = f(all_measures[start_comb_idx: end_comb_idx+1]) #including ending index for interpolation\n",
    "            print(\"After interpolation:\", all_measures[start_comb_idx: end_comb_idx+1],\n",
    "                 \"maps to \", y1)\n",
    "\n",
    "            #assign\n",
    "            extrap_df[v].loc[start_comb_idx: end_comb_idx] = y1\n",
    "            print(\"currently, dataframe is \\n\", extrap_df[v])\n",
    "            \n",
    "            '''\n",
    "            2. Perform forward extrapolation \n",
    "            '''\n",
    "            print(\"\\n&&&&\\nExtrapolation\")\n",
    "            #start extrapolation at last value of current block\n",
    "            forward_extrap_start = x[blocks[i][-1]]\n",
    "            #end extrapolation on first value of next block or end of piece\n",
    "            if i < len(blocks)-1:\n",
    "                forward_extrap_end = x[blocks[i+1][0]]\n",
    "            else:\n",
    "                forward_extrap_end = all_measures[-1]\n",
    "            #extrapolate vals\n",
    "            \n",
    "            forward_start_idx = all_measures.index(forward_extrap_start)\n",
    "            forward_end_idx = all_measures.index(forward_extrap_end)\n",
    "            #measures slice\n",
    "            forward_x = all_measures[forward_start_idx: forward_end_idx+1 ] #include end\n",
    "            forward_y = f(forward_x)\n",
    "            print(\"In forward extrapolation: mapped\", forward_x, \"to\", forward_y)\n",
    "            #make start 0 to get differences \n",
    "            forward_y = forward_y - forward_y[0]\n",
    "            \n",
    "            #add x y pairs to dictionary\n",
    "            forward_extrapolation_lst.append({'startval':forward_start_idx, \n",
    "                                              'endval':forward_end_idx,\n",
    "                                              'y':forward_y})\n",
    "            '''\n",
    "            2. Perform backward extrapolation \n",
    "            '''\n",
    "            #start at beginning of piece or end of last block\n",
    "            if i > 0:\n",
    "                backward_extrap_start = x[blocks[i-1][-1]]\n",
    "            else:\n",
    "                backward_extrap_start =all_measures[0]\n",
    "            #end on first beat of this block\n",
    "            backward_extrap_end =    x[blocks[i][0]]\n",
    "            \n",
    "            backward_start_idx = all_measures.index(backward_extrap_start)\n",
    "            backward_end_idx = all_measures.index(backward_extrap_end)\n",
    "            \n",
    "            \n",
    "            backward_x = all_measures[backward_start_idx: backward_end_idx+1] #include end\n",
    "            backward_y = f(backward_x)\n",
    "            print(\"In backward extrapolation: mapped\", backward_x, \"to\", backward_y)\n",
    "            backward_y = backward_y - backward_y[0]\n",
    "            \n",
    "            backward_extrapolation_lst.append({'startval':backward_start_idx,\n",
    "                                               'endval':backward_end_idx , \n",
    "                                               'y':backward_y})\n",
    " \n",
    "        print(\"\\n\\n@@@@@@@@@@@@@@@\\n Part 3: filling in long NAN values\")\n",
    "\n",
    "        '''\n",
    "        Decide final extrapolated values\n",
    "        '''\n",
    "        #Fill beginning and ending values\n",
    "        beginning = backward_extrapolation_lst[0]  \n",
    "        print('beginning dct', beginning)\n",
    "        # TODO: this should move backward from last existing value\n",
    "        extrap_df[v].loc[beginning['startval']: beginning['endval']] = (\n",
    "            extrap_df[v].loc[beginning['endval']]- beginning['y'][-1])+ beginning['y']\n",
    "        \n",
    "        print(extrap_df[v].loc[beginning['endval']]- beginning['y'][-1])\n",
    "        print(beginning['y'])\n",
    "        backward_extrapolation_lst.pop(0)\n",
    "        \n",
    "        ending = forward_extrapolation_lst[-1] \n",
    "        #for ending values, add differences on last existing value\n",
    "        extrap_df[v].loc[ending['startval']: ending['endval']] = ending['y'] +extrap_df[v].loc[ending['startval']]\n",
    "        forward_extrapolation_lst.pop(-1)      \n",
    "        \n",
    "        print(\"dataframe\\n\", extrap_df[v])\n",
    "        \n",
    "        print(\"\\n\\n$$$$$$$$$$\\nExtrapolating long rests and moving forward\\n\")\n",
    "        \n",
    "        \n",
    "        print('backward extrap info\\n', backward_extrapolation_lst[0:3])\n",
    "        print('forward extrap info\\n', forward_extrapolation_lst[0:3])\n",
    "        #Rest of long rests:\n",
    "        for i in range(len(backward_extrapolation_lst)):\n",
    "            print(\"SHOULD BE SAME\")\n",
    "            print(\"starts\", backward_extrapolation_lst[i]['startval'], \n",
    "                 forward_extrapolation_lst[i]['startval'])\n",
    "            print(\"ends\", backward_extrapolation_lst[i]['endval'], \n",
    "                 forward_extrapolation_lst[i]['endval'])\n",
    "            \n",
    "            forward_y = forward_extrapolation_lst[i]['y']\n",
    "            backward_y = backward_extrapolation_lst[i]['y']\n",
    "            start_idx = forward_extrapolation_lst[i]['startval']\n",
    "            end_idx = forward_extrapolation_lst[i]['endval']\n",
    "            start_frame = extrap_df[v].loc[start_idx]\n",
    "            end_frame = extrap_df[v].loc[end_idx]\n",
    "            print(\"start frame\", start_frame)\n",
    "            print(\"end_frame\", end_frame)\n",
    "            '''\n",
    "            Option 1: average:  \n",
    "            '''\n",
    "            print(\"forward_y\", forward_y )\n",
    "            print(\"backward_y\",backward_y)\n",
    "            y_extrap = (np.array(forward_y) + np.array(backward_y)) /2\n",
    "            #add to last value\n",
    "            y_extrap = y_extrap + start_frame\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            Perform assignment of nan values and shifting of values after\n",
    "            '''\n",
    "       \n",
    "            #assign extrapolation to NAN values\n",
    "            print(\"y_extrap\\n\", y_extrap)\n",
    "            print(\"putting values at \", extrap_df[v].loc[start_idx: end_idx])\n",
    "            extrap_df[v].loc[start_idx: end_idx] = y_extrap\n",
    "            print(\"df\\n\", extrap_df[v])\n",
    "            #shift forward previous values\n",
    "            shift = extrap_df[v].loc[end_idx]-end_frame\n",
    "            print(\"next section should shift by \", shift)\n",
    "            if i< len(backward_extrapolation_lst) - 1: \n",
    "                next_start_idx = forward_extrapolation_lst[i+1]['startval']\n",
    "            else: \n",
    "                next_start_idx = len(all_measures)\n",
    "                \n",
    "            print(\"inserting into\\n\", extrap_df[v].loc[end_idx: next_start_idx])\n",
    "            print(\"end idx\", end_idx)\n",
    "            print(\"next start idx\", next_start_idx)\n",
    "            extrap_df[v].loc[end_idx+1: next_start_idx] = extrap_df[v].loc[end_idx+1: next_start_idx]+shift\n",
    "\n",
    "    return extrap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_measures = list(comb_df['eval_measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_comb_df = make_extrapolate_df(comb_df, df_dct, all_measures, method = \"avg\")\n",
    "filled_comb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-egypt",
   "metadata": {},
   "source": [
    "# Other utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_new_stft(df_dct, optimized_df, instrument_lst):\n",
    "    df_dct1 = df_dct.copy()\n",
    "    for v in instrument_lst:\n",
    "\n",
    "        frames_df=  optimized_df[['eval_measure']].copy()\n",
    "        frames_df['New STFT frames']=optimized_df[v].astype(int) #was hardcoded oboe_1\n",
    "        frames_df[\"new_times\"] = stft_frames_to_seconds(frames_df['New STFT frames'])\n",
    "        frames_df= frames_df.set_index('eval_measure')\n",
    "\n",
    "        #join\n",
    "        df_dct1[v]=df_dct[v].set_index('eval_measure').join(frames_df)\n",
    "    return df_dct1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tempos(x0, x_gd, del_m_series, start=0, stop=None, ylow=None, yhigh=None, show_rates=False):\n",
    "    for p in range(n_p):\n",
    "\n",
    "        d = np.diff(x0[:,p])\n",
    "\n",
    "\n",
    "        e = np.diff(x_gd[:,p])\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title(instrument_lst[p])\n",
    "        plt.plot((d/del_m_series)[start:stop], label='old tempo')\n",
    "        plt.plot((e/del_m_series)[start:stop], label = 'new ')\n",
    "        if show_rates == True:\n",
    "            print(\"Rate values\", (e/del_m_series)[start:stop])\n",
    "        plt.ylim([ylow, yhigh])\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-reception",
   "metadata": {},
   "source": [
    "# Loss, loss derivative, and loss hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-tutorial",
   "metadata": {},
   "source": [
    "## $L = L_e+ L_s + L_a + L_r$\n",
    "\n",
    "# $L_g = \\sum_{i=1}^{n_p}\\sum_{j=1}^{n_b} (t_{i,j}-t^g_{i,j} )^2$\n",
    "\n",
    "$\\frac{\\partial L_a}{\\partial t_{i,j}} =  2(t_{i,j}-t^a_{i,j} )$\n",
    "\n",
    "$\\frac{\\partial ^2 L_a}{\\partial t_{i,j}^2} =  2$\n",
    "\n",
    "where $t^a_{i,j}$ the timepoint of a close-to-perfect version\n",
    "\n",
    "\n",
    "$L_r = \\sum_{i=1}^{n_p}\\sum_{j=1}^{n_b} (\\frac{t_{i,j+1}-t_{i,j}}{m_{j+1}-m_{j}} - R_0 )^2$\n",
    "\n",
    "where $R_0$ is average desired rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-lesbian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_der2_sq():\n",
    "    der2_sq = np.ones_like(x0)* (a*(n_p-1)**2+ d*2)#ensemble and absolute loss\n",
    "    \n",
    "    #first range: I(b<n_b-1)\n",
    "    i_del_m  = inv_del_m_series[:-1,:]\n",
    "    #print('first range\\n', 2*c*i_del_m * (i_del_m_p1 * del_t_p1 - i_del_m*del_t))\n",
    "    der2_sq[ 0: n_b-2, : ] += 2*c*i_del_m **2\n",
    "\n",
    "    #second range: I(1 < b < n_b)\n",
    "\n",
    "    i_del_m = inv_del_m_series[1:, :]\n",
    "    i_del_m_m1 = inv_del_m_series[:-1, :]\n",
    "\n",
    "    der2_sq[1:-1, :] += 2*c*(-1*i_del_m - i_del_m_m1)**2\n",
    "    \n",
    "    #print('sec range\\n', 2*c*(-1*i_del_m - i_del_m_m1) * (\n",
    "     #   i_del_m*del_t - i_del_m_m1*del_t_m1))    \n",
    "    #third range: b > 1\n",
    "\n",
    "    i_del_m_m1 = inv_del_m_series[1:, :]\n",
    "\n",
    "    der2_sq[2:, :] += 2*c*(i_del_m_m1)**2\n",
    "    \n",
    "    return der2_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_der2_p1():\n",
    "    \n",
    "    #print('initial series', inv_del_m_series)\n",
    "    der2_p1 = np.zeros_like(x0)\n",
    "    \n",
    "\n",
    "    #first range: I(b<n_b-1)\n",
    "\n",
    "    i_del_m_p1 = inv_del_m_series[1:,:]\n",
    "    i_del_m  = inv_del_m_series[:-1,:]\n",
    "    #print('first range\\n', 2*c*i_del_m * (i_del_m_p1 * del_t_p1 - i_del_m*del_t))\n",
    "    der2_p1[ 0: n_b-2, : ] += 2*c*(-1*i_del_m_p1 - i_del_m) * i_del_m\n",
    "\n",
    "    #second range: I(1 < b < n_b)\n",
    "    i_del_m = inv_del_m_series[1:, :]\n",
    "    i_del_m_m1 = inv_del_m_series[:-1, :]\n",
    "    der2_p1[1:-1, :] += 2*c*i_del_m*(-1*i_del_m - i_del_m_m1)\n",
    "\n",
    "    #third range: b > 1 -- no values\n",
    "\n",
    "    return der2_p1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_der2_m1():\n",
    "    \n",
    "    der2_m1 = np.zeros_like(x0)\n",
    "    \n",
    "    #first range: I(b<n_b-1) -- no values\n",
    "\n",
    "    #second range: I(1 < b < n_b)\n",
    "    i_del_m = inv_del_m_series[1:, :]\n",
    "    i_del_m_m1 = inv_del_m_series[:-1, :]\n",
    "\n",
    "    der2_m1[1:-1, :] += 2*c* i_del_m_m1*(-1*i_del_m - i_del_m_m1) \n",
    "    \n",
    "    #print('sec range\\n', 2*c*(-1*i_del_m - i_del_m_m1) * (\n",
    "     #   i_del_m*del_t - i_del_m_m1*del_t_m1))    \n",
    "    #third range: b > 1\n",
    "\n",
    "    i_del_m_m1 = inv_del_m_series[1:, :]\n",
    "    i_del_m_m2 = inv_del_m_series[:-1, :]\n",
    "\n",
    "    der2_m1[2:, :] += 2*c*(-1*i_del_m_m1 - i_del_m_m2)*i_del_m_m1\n",
    "    \n",
    "    #print('third range\\n', 2*c*(i_del_m_m1) * (\n",
    "     #   i_del_m_m1*del_t_m1 - i_del_m_m2*del_t_m2))     \n",
    "    return der2_m1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_der2_p2():\n",
    "    \n",
    "\n",
    "    der2_p2 = np.zeros_like(x0)\n",
    "    \n",
    "\n",
    "    #first range: I(b<n_b-1)\n",
    "\n",
    "    i_del_m_p1 = inv_del_m_series[1:,:]\n",
    "    i_del_m  = inv_del_m_series[:-1,:]\n",
    "    der2_p2[ 0: n_b-2, : ] += 2*c*i_del_m * i_del_m_p1\n",
    "\n",
    "    #second range: I(1 < b < n_b) -- no values\n",
    "\n",
    "    #third range: b > 1  -- no values\n",
    "\n",
    "    return der2_p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_der2_m2():\n",
    "    \n",
    "\n",
    "    der2_m2 = np.zeros_like(x0)\n",
    "    \n",
    "\n",
    "    #first range: I(b<n_b-1)  -- no values\n",
    "\n",
    "    #second range: I(1 < b < n_b) -- no values\n",
    "    \n",
    "    #third range: b > 1\n",
    "\n",
    "    i_del_m_m1 = inv_del_m_series[1:, :]\n",
    "    i_del_m_m2 = inv_del_m_series[:-1, :]\n",
    "\n",
    "    der2_m2[2:, :] += 2*c*(i_del_m_m1) * (i_del_m_m2)\n",
    "    \n",
    "    #print('third range\\n', 2*c*(i_del_m_m1) * (\n",
    "     #   i_del_m_m1*del_t_m1 - i_del_m_m2*del_t_m2))     \n",
    "    return der2_m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(x):\n",
    "    \n",
    "    #print(x)\n",
    "    #calculate ensemble loss\n",
    "    x = x.reshape(n_b, n_p)\n",
    "    \n",
    "    loss = np.zeros_like(x)\n",
    "    \n",
    "    '''\n",
    "    Ensemble Loss\n",
    "    '''\n",
    "    ensemble_loss = np.ones_like(x)\n",
    "    for p in range(n_p):\n",
    "        #print(\"\\n\\n*******\\n\", p)\n",
    "        instr_slice = x[:,other_voices_dct[p]] #initialize\n",
    "        #print(instr_slice)\n",
    "        cur_val = x[:, p:p+1] \n",
    "        #print(instr_slice)\n",
    "        instr_sum_arr = a * np.sum((cur_val - instr_slice)**2, axis=1)  \n",
    "        \n",
    "        #instr_sum_arr = 2 * a * np.sum((cur_val - instr_slice)**2, axis=1) \n",
    "        #instr_sum_arr = 2 * a * np.sum((x-cur_val)**2, axis=1) \n",
    "        #print(instr_sum_arr)\n",
    "        \n",
    "        ensemble_loss[:,p]= instr_sum_arr\n",
    "     \n",
    "    #print(\"ensemble loss\\n\", loss)\n",
    "    loss += ensemble_loss\n",
    "    del_t_series =  np.diff(x, axis=0)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Stretch Loss\n",
    "    '''\n",
    "\n",
    "    del_t = del_t_series[1:, :]\n",
    "    del_t_m1 = del_t_series[:-1, :]\n",
    "\n",
    "    i_del_m = inv_del_m_series[1:, :]\n",
    "    i_del_m_m1 = inv_del_m_series[:-1, :]\n",
    "\n",
    "    loss[1:-1, :] += c*(i_del_m*del_t - i_del_m_m1*del_t_m1)**2\n",
    "    #print(\"stretch loss I(1 < b < n_b) \\n\", loss)    \n",
    "    \n",
    "    #print(\"stretch loss I(b>1) \\n\", loss)  \n",
    "    \n",
    "    '''\n",
    "    \"Absolute\" Loss\n",
    "    '''\n",
    "    \n",
    "    abs_loss = d*(x-abs_times_arr)**2\n",
    "    \n",
    "    loss += abs_loss\n",
    "    \n",
    "    print(np.sum(loss))\n",
    "    return np.sum(loss)\n",
    "    #return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(x):\n",
    "    \n",
    "\n",
    "    #calculate ensemble loss\n",
    "    x = x.reshape(n_b, n_p)\n",
    "    der = np.zeros_like(x)\n",
    "    \n",
    "    '''\n",
    "    Ensemble Loss derivative\n",
    "    '''\n",
    "    \n",
    "    ensemble_der = np.ones_like(x)\n",
    "    for p in range(n_p):\n",
    "        #print(\"\\n\\n*******\\n\", p)\n",
    "        instr_slice = x[:,other_voices_dct[p]] #initialize\n",
    "        #print(instr_slice)\n",
    "        cur_val = x[:, p:p+1] \n",
    "        #print(instr_slice)\n",
    "        #instr_sum_arr = 2*a*np.sum(cur_val - instr_slice, axis=1)  \n",
    "        instr_sum_arr = 4*a* np.sum(cur_val-instr_slice , axis=1) \n",
    "        #print(instr_sum_arr)\n",
    "        ensemble_der[:,p]= instr_sum_arr\n",
    "     \n",
    "    der += ensemble_der\n",
    "    #print('ensemble grad\\n', der)\n",
    "    \n",
    "    '''\n",
    "    Stretch Loss Derivative\n",
    "    '''\n",
    "    del_t_series =  np.diff(x, axis=0)\n",
    "    \n",
    "    #print('del_t_serie\\n', del_t_series)\n",
    "    #print('inv_del_m_series', inv_del_m_series)\n",
    "    #first range: I(b<n_b-1)\n",
    "    del_t_p1 = del_t_series[1:,:]\n",
    "    del_t = del_t_series[:-1,:]\n",
    "    i_del_m_p1 = inv_del_m_series[1:,:]\n",
    "    i_del_m  = inv_del_m_series[:-1,:]\n",
    "    #print('first range\\n', 2*c*i_del_m * (i_del_m_p1 * del_t_p1 - i_del_m*del_t))\n",
    "    der[ 0: n_b-2, : ] += 2*c*i_del_m * (i_del_m_p1 * del_t_p1 - i_del_m*del_t)\n",
    "\n",
    "    #second range: I(1 < b < n_b)\n",
    "    \n",
    "\n",
    "    del_t = del_t_series[1:, :]\n",
    "    del_t_m1 = del_t_series[:-1, :]\n",
    "\n",
    "    i_del_m = inv_del_m_series[1:, :]\n",
    "    i_del_m_m1 = inv_del_m_series[:-1, :]\n",
    "\n",
    "    der[1:-1, :] += 2*c*(-1*i_del_m - i_del_m_m1) * (\n",
    "        i_del_m*del_t - i_del_m_m1*del_t_m1)\n",
    "    \n",
    "    #print('sec range\\n', 2*c*(-1*i_del_m - i_del_m_m1) * (\n",
    "     #   i_del_m*del_t - i_del_m_m1*del_t_m1))    \n",
    "    #third range: b > 1\n",
    "    \n",
    "    del_t_m1 = del_t_series[1:, :]\n",
    "    del_t_m2 = del_t_series[:-1, :]\n",
    "\n",
    "    i_del_m_m1 = inv_del_m_series[1:, :]\n",
    "    i_del_m_m2 = inv_del_m_series[:-1, :]\n",
    "\n",
    "    der[2:, :] += 2*c*(i_del_m_m1) * (\n",
    "        i_del_m_m1*del_t_m1 - i_del_m_m2*del_t_m2)\n",
    "    \n",
    "    #print('third range\\n', 2*c*(i_del_m_m1) * (\n",
    "     #   i_del_m_m1*del_t_m1 - i_del_m_m2*del_t_m2))     \n",
    "        \n",
    "    '''\n",
    "    'Absolute' Loss derivative\n",
    "    '''\n",
    "    \n",
    "    der += d*2*(x-abs_times_arr)\n",
    "   \n",
    "    return der.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_hessp(x, in_vec):\n",
    "    \n",
    "    #print('size of in_vec is ', in_vec.shape)\n",
    "    in_vec = in_vec.reshape(n_b, n_p) #do this so indices match\n",
    "    x = x.reshape(n_b, n_p)\n",
    "    #print('x is ', x)\n",
    "    Hp = np.zeros_like(x) #we will flatten this later\n",
    "    #print(\"Hp\", Hp)\n",
    "    #print('element', Hp[0][0])\n",
    "\n",
    "    \n",
    "    for p in range(n_p): #which part\n",
    "        for b in range(n_b): #which beat\n",
    "            \n",
    "            Hp[b][p] = 0\n",
    "            \n",
    "            #catch cross terms\n",
    "            for not_p in other_voices_dct[p]: \n",
    "                Hp[b][p]+= der2_part_const* in_vec[b][not_p] #b is cur position, not_p gives block\n",
    "                \n",
    "                \n",
    "            #Always include diagonal term\n",
    "            Hp[b][p] += der2_sq[b][p]*in_vec[b][p] \n",
    " \n",
    "            #Range-dependent conditionals for stretch terms\n",
    "\n",
    "            if b < n_b-2:\n",
    "                Hp[b][p] += der2_m2[b+2][p]*in_vec[b+2][p]\n",
    "                \n",
    "            if b < n_b-1:\n",
    "                Hp[b][p] += der2_m1[b+1][p]*in_vec[b+1][p]\n",
    "                \n",
    "            if b > 0:\n",
    "                Hp[b][p] += der2_p1[b-1][p]*in_vec[b-1][p]\n",
    "                \n",
    "            if b > 1: \n",
    "                Hp[b][p] += der2_p2[b-2][p]*in_vec[b-2][p]\n",
    "   \n",
    "    #print('Final', Hp)  \n",
    "    return Hp.flatten()    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-explanation",
   "metadata": {},
   "source": [
    "## First pass: Just ensemble loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-withdrawal",
   "metadata": {},
   "source": [
    "# Alt First pass: metronomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_timing_scheme = (filled_comb_df['eval_measure']-1)*700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_time_df = filled_comb_df.copy()\n",
    "for p in instrument_lst:\n",
    "    metro_time_df[p]=new_timing_scheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_time_df[instrument_lst].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-million",
   "metadata": {},
   "source": [
    "## New loss function adds stretch loss to absolute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = filled_comb_df[instrument_lst].to_numpy()\n",
    "a =.0000005#.000001#0.000001\n",
    "c = .9999999#.999999\n",
    "d=.0000005\n",
    "n_p = x0.shape[1]\n",
    "n_b = x0.shape[0]\n",
    "abs_times_arr = metro_time_df[instrument_lst].to_numpy()\n",
    "#abs_times_arr = ensemble_res ['x'].reshape(n_b, n_p)\n",
    "del_t_arr = np.diff(x0, axis=0)\n",
    "inv_del_m_series = 1/del_t_arr\n",
    "other_voices_dct = {0:[1,2,3,4,5,6,7], \n",
    "                    1:[0,2,3,4,5,6,7], \n",
    "                    2:[0,1,3,4,5,6,7],\n",
    "                    3:[0,1,2,4,5,6,7],\n",
    "                    4:[0,1,2,3,5,6,7],\n",
    "                    5:[0,1,2,3,4,6,7],\n",
    "                    6:[0,1,2,3,4,5,7],\n",
    "                    7:[0,1,2,3,4,5,6]}\n",
    "\n",
    "\n",
    "#Newton's method second derivatives\n",
    "der2_part_const = -1*a*(n_p-1)\n",
    "\n",
    "\n",
    "der2_p2 = make_der2_p2()\n",
    "der2_p1 = make_der2_p1()\n",
    "der2_sq = make_der2_sq()\n",
    "der2_m1= make_der2_m1()\n",
    "der2_m2= make_der2_m2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "a+c+d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-active",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "final_res = minimize(loss_fun, x0, method='Newton-CG',\n",
    "               jac=loss_derivative, hessp=loss_hessp,\n",
    "               options={  'disp': True})\n",
    "\n",
    "end = time.time()\n",
    "print('seconds elapsed', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_res['x'].reshape(n_b, n_p),columns = instrument_lst, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df+20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['eval_measure']= filled_comb_df['eval_measure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"grid_stretch.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stretch = assign_new_stft(df_dct, final_df, instrument_lst)\n",
    "stretched_parts = create_final_parts(ensemble_audio_lst,final_stretch,instrument_lst, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_parts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_full_audio=sum_parts(stretched_parts)\n",
    "ipd.Audio(stretched_full_audio, rate=sr_ensemble) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_mix_parts(stretched_parts, nonvar_mix, folder_path = \"./parts_audio/grid_stretch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "write(\"whole_audio/FINALPARAMensemble_stretch.99999ensemble.000001metro.000009.wav\", sr_ensemble, stretched_full_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-sentence",
   "metadata": {},
   "source": [
    "# Visualize and find error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time_arr = final_res['x'].reshape(n_b, n_p)\n",
    "final_time_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_times = np.array([np.sum(final_res['x'].reshape(n_b, n_p), axis=1)/num_voices]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-import",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_difs = final_res['x'].reshape(n_b, n_p) - avg_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"3 part loss - Error distribution\")\n",
    "plt.hist(all_difs.flatten(), bins=100)\n",
    "plt.xlabel(\"Difference from mean times (in frames)\")\n",
    "plt.ylabel(\"Number of timepoints\")\n",
    "plt.show()\n",
    "print(\"Inter quartile range is\", iqr(all_difs.flatten()))\n",
    "print(\"Maximum deviation\", np.max(np.abs(all_difs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-waterproof",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-bikini",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-charger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-utilization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-mobile",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-flash",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Octover 15 demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlsp-env",
   "language": "python",
   "name": "mlsp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
